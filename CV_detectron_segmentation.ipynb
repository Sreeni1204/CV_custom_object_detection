{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CV_test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PW2Pg2hQjq9S","colab_type":"code","outputId":"1ca59566-9215-4362-f1f2-6907b26cbf95","executionInfo":{"status":"ok","timestamp":1575581601791,"user_tz":-60,"elapsed":20782,"user":{"displayName":"sreeni vasa hv","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGRGILT9ymLBiio6LAzf9v1Yur0IFEmPAOx9tG-0I=s64","userId":"10395202719377635148"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F-ItC0_JrGNs","colab_type":"code","outputId":"a7cdd09f-36ca-4ef2-8507-a8a1e3fdda68","executionInfo":{"status":"ok","timestamp":1575581620944,"user_tz":-60,"elapsed":11638,"user":{"displayName":"sreeni vasa hv","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGRGILT9ymLBiio6LAzf9v1Yur0IFEmPAOx9tG-0I=s64","userId":"10395202719377635148"}},"colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["!pip install -U torch torchvision\n","!pip install git+https://github.com/facebookresearch/fvcore.git\n","import torch, torchvision\n","torch.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already up-to-date: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n","Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n","Collecting git+https://github.com/facebookresearch/fvcore.git\n","  Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-jc7pw5hc\n","  Running command git clone -q https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-jc7pw5hc\n","Collecting yacs>=0.1.6\n","  Downloading https://files.pythonhosted.org/packages/2f/51/9d613d67a8561a0cdf696c3909870f157ed85617fea3cff769bb7de09ef7/yacs-0.1.6-py3-none-any.whl\n","Collecting pyyaml>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/c9/e5be955a117a1ac548cdd31e37e8fd7b02ce987f9655f5c7563c656d5dcb/PyYAML-5.2.tar.gz (265kB)\n","\u001b[K     |████████████████████████████████| 266kB 7.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (4.28.1)\n","Collecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (1.1.0)\n","Building wheels for collected packages: fvcore, pyyaml\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1-cp36-none-any.whl size=50354 sha256=8022f6671c06664c16b7028ef6d5813e806abdde6ba5f6621349a16a00aaf40e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-g_e1xkda/wheels/48/53/79/3c6485543a4455a0006f5db590ab9957622b6227011941de06\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.2-cp36-cp36m-linux_x86_64.whl size=44209 sha256=f8130e6a47b6beec4a7ab85ccc482ed0966e9282f21ebf46b1bd19915baf0a16\n","  Stored in directory: /root/.cache/pip/wheels/54/b7/c7/2ada654ee54483c9329871665aaf4a6056c3ce36f29cf66e67\n","Successfully built fvcore pyyaml\n","Installing collected packages: pyyaml, yacs, portalocker, fvcore\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed fvcore-0.1 portalocker-1.5.2 pyyaml-5.2 yacs-0.1.6\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'1.3.1'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"EqOIbyj1rQpn","colab_type":"code","outputId":"3c734667-ff13-402d-e3bb-923721e2032d","executionInfo":{"status":"ok","timestamp":1575581804336,"user_tz":-60,"elapsed":180981,"user":{"displayName":"sreeni vasa hv","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGRGILT9ymLBiio6LAzf9v1Yur0IFEmPAOx9tG-0I=s64","userId":"10395202719377635148"}},"colab":{"base_uri":"https://localhost:8080/","height":852}},"source":["!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n","!pip install -e detectron2_repo"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'detectron2_repo'...\n","remote: Enumerating objects: 1653, done.\u001b[K\n","Receiving objects:   0% (1/1653)   \rReceiving objects:   1% (17/1653)   \rReceiving objects:   2% (34/1653)   \rReceiving objects:   3% (50/1653)   \rReceiving objects:   4% (67/1653)   \rReceiving objects:   5% (83/1653)   \rReceiving objects:   6% (100/1653)   \rReceiving objects:   7% (116/1653)   \rReceiving objects:   8% (133/1653)   \rReceiving objects:   9% (149/1653)   \rReceiving objects:  10% (166/1653)   \rReceiving objects:  11% (182/1653)   \rReceiving objects:  12% (199/1653)   \rReceiving objects:  13% (215/1653)   \rReceiving objects:  14% (232/1653)   \rReceiving objects:  15% (248/1653)   \rReceiving objects:  16% (265/1653)   \rReceiving objects:  17% (282/1653)   \rReceiving objects:  18% (298/1653)   \rReceiving objects:  19% (315/1653)   \rReceiving objects:  20% (331/1653)   \rReceiving objects:  21% (348/1653)   \rReceiving objects:  22% (364/1653)   \rReceiving objects:  23% (381/1653)   \rReceiving objects:  24% (397/1653)   \rReceiving objects:  25% (414/1653)   \rReceiving objects:  26% (430/1653)   \rReceiving objects:  27% (447/1653)   \rReceiving objects:  28% (463/1653)   \rReceiving objects:  29% (480/1653)   \rReceiving objects:  30% (496/1653)   \rReceiving objects:  31% (513/1653)   \rReceiving objects:  32% (529/1653)   \rReceiving objects:  33% (546/1653)   \rReceiving objects:  34% (563/1653)   \rReceiving objects:  35% (579/1653)   \rReceiving objects:  36% (596/1653)   \rReceiving objects:  37% (612/1653)   \rReceiving objects:  38% (629/1653)   \rReceiving objects:  39% (645/1653)   \rReceiving objects:  40% (662/1653)   \rReceiving objects:  41% (678/1653)   \rReceiving objects:  42% (695/1653)   \rReceiving objects:  43% (711/1653)   \rReceiving objects:  44% (728/1653)   \rReceiving objects:  45% (744/1653)   \rReceiving objects:  46% (761/1653)   \rReceiving objects:  47% (777/1653)   \rReceiving objects:  48% (794/1653)   \rReceiving objects:  49% (810/1653)   \rReceiving objects:  50% (827/1653)   \rReceiving objects:  51% (844/1653)   \rReceiving objects:  52% (860/1653)   \rReceiving objects:  53% (877/1653)   \rReceiving objects:  54% (893/1653)   \rReceiving objects:  55% (910/1653)   \rReceiving objects:  56% (926/1653)   \rReceiving objects:  57% (943/1653)   \rReceiving objects:  58% (959/1653)   \rReceiving objects:  59% (976/1653)   \rReceiving objects:  60% (992/1653)   \rReceiving objects:  61% (1009/1653)   \rReceiving objects:  62% (1025/1653)   \rReceiving objects:  63% (1042/1653)   \rReceiving objects:  64% (1058/1653)   \rReceiving objects:  65% (1075/1653)   \rReceiving objects:  66% (1091/1653)   \rReceiving objects:  67% (1108/1653)   \rReceiving objects:  68% (1125/1653)   \rReceiving objects:  69% (1141/1653)   \rReceiving objects:  70% (1158/1653)   \rReceiving objects:  71% (1174/1653)   \rReceiving objects:  72% (1191/1653)   \rReceiving objects:  73% (1207/1653)   \rReceiving objects:  74% (1224/1653)   \rReceiving objects:  75% (1240/1653)   \rReceiving objects:  76% (1257/1653)   \rReceiving objects:  77% (1273/1653)   \rReceiving objects:  78% (1290/1653)   \rReceiving objects:  79% (1306/1653)   \rReceiving objects:  80% (1323/1653)   \rremote: Total 1653 (delta 0), reused 0 (delta 0), pack-reused 1653\u001b[K\n","Receiving objects:  81% (1339/1653)   \rReceiving objects:  82% (1356/1653)   \rReceiving objects:  83% (1372/1653)   \rReceiving objects:  84% (1389/1653)   \rReceiving objects:  85% (1406/1653)   \rReceiving objects:  86% (1422/1653)   \rReceiving objects:  87% (1439/1653)   \rReceiving objects:  88% (1455/1653)   \rReceiving objects:  89% (1472/1653)   \rReceiving objects:  90% (1488/1653)   \rReceiving objects:  91% (1505/1653)   \rReceiving objects:  92% (1521/1653)   \rReceiving objects:  93% (1538/1653)   \rReceiving objects:  94% (1554/1653)   \rReceiving objects:  95% (1571/1653)   \rReceiving objects:  96% (1587/1653)   \rReceiving objects:  97% (1604/1653)   \rReceiving objects:  98% (1620/1653)   \rReceiving objects:  99% (1637/1653)   \rReceiving objects: 100% (1653/1653)   \rReceiving objects: 100% (1653/1653), 1.68 MiB | 27.23 MiB/s, done.\n","Resolving deltas:   0% (0/1024)   \rResolving deltas:   1% (11/1024)   \rResolving deltas:   2% (24/1024)   \rResolving deltas:   3% (31/1024)   \rResolving deltas:   4% (51/1024)   \rResolving deltas:   5% (55/1024)   \rResolving deltas:   6% (66/1024)   \rResolving deltas:   7% (73/1024)   \rResolving deltas:   8% (88/1024)   \rResolving deltas:   9% (93/1024)   \rResolving deltas:  10% (105/1024)   \rResolving deltas:  11% (116/1024)   \rResolving deltas:  12% (124/1024)   \rResolving deltas:  13% (134/1024)   \rResolving deltas:  14% (145/1024)   \rResolving deltas:  15% (154/1024)   \rResolving deltas:  16% (164/1024)   \rResolving deltas:  17% (177/1024)   \rResolving deltas:  18% (189/1024)   \rResolving deltas:  19% (202/1024)   \rResolving deltas:  20% (205/1024)   \rResolving deltas:  21% (217/1024)   \rResolving deltas:  22% (228/1024)   \rResolving deltas:  23% (237/1024)   \rResolving deltas:  24% (247/1024)   \rResolving deltas:  25% (264/1024)   \rResolving deltas:  26% (269/1024)   \rResolving deltas:  27% (277/1024)   \rResolving deltas:  28% (288/1024)   \rResolving deltas:  29% (297/1024)   \rResolving deltas:  30% (309/1024)   \rResolving deltas:  31% (318/1024)   \rResolving deltas:  32% (329/1024)   \rResolving deltas:  33% (339/1024)   \rResolving deltas:  34% (350/1024)   \rResolving deltas:  35% (361/1024)   \rResolving deltas:  36% (369/1024)   \rResolving deltas:  39% (408/1024)   \rResolving deltas:  40% (415/1024)   \rResolving deltas:  41% (421/1024)   \rResolving deltas:  42% (431/1024)   \rResolving deltas:  43% (442/1024)   \rResolving deltas:  44% (456/1024)   \rResolving deltas:  45% (464/1024)   \rResolving deltas:  46% (480/1024)   \rResolving deltas:  47% (483/1024)   \rResolving deltas:  48% (494/1024)   \rResolving deltas:  49% (503/1024)   \rResolving deltas:  50% (512/1024)   \rResolving deltas:  51% (523/1024)   \rResolving deltas:  52% (535/1024)   \rResolving deltas:  53% (546/1024)   \rResolving deltas:  54% (553/1024)   \rResolving deltas:  55% (564/1024)   \rResolving deltas:  57% (592/1024)   \rResolving deltas:  58% (595/1024)   \rResolving deltas:  59% (608/1024)   \rResolving deltas:  60% (615/1024)   \rResolving deltas:  61% (627/1024)   \rResolving deltas:  62% (635/1024)   \rResolving deltas:  63% (651/1024)   \rResolving deltas:  64% (657/1024)   \rResolving deltas:  65% (669/1024)   \rResolving deltas:  66% (678/1024)   \rResolving deltas:  67% (695/1024)   \rResolving deltas:  68% (697/1024)   \rResolving deltas:  69% (713/1024)   \rResolving deltas:  70% (721/1024)   \rResolving deltas:  71% (736/1024)   \rResolving deltas:  72% (740/1024)   \rResolving deltas:  74% (758/1024)   \rResolving deltas:  76% (786/1024)   \rResolving deltas:  77% (790/1024)   \rResolving deltas:  78% (803/1024)   \rResolving deltas:  79% (813/1024)   \rResolving deltas:  80% (826/1024)   \rResolving deltas:  81% (831/1024)   \rResolving deltas:  83% (851/1024)   \rResolving deltas:  85% (875/1024)   \rResolving deltas:  88% (907/1024)   \rResolving deltas:  89% (912/1024)   \rResolving deltas:  90% (922/1024)   \rResolving deltas:  91% (932/1024)   \rResolving deltas:  92% (944/1024)   \rResolving deltas:  93% (954/1024)   \rResolving deltas:  94% (966/1024)   \rResolving deltas:  96% (986/1024)   \rResolving deltas: 100% (1024/1024)   \rResolving deltas: 100% (1024/1024), done.\n","Obtaining file:///content/detectron2_repo\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.1.0)\n","Collecting Pillow>=6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5c/0e94e689de2476c4c5e644a3bd223a1c1b9e2bdb7c510191750be74fa786/Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 4.8MB/s \n","\u001b[?25hRequirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.1.6)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.8.6)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (3.1.2)\n","Collecting tqdm>4.29.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/13/cd55c23e3e158ed5b87cae415ee3844fc54cb43803fa3a0a064d23ecb883/tqdm-4.40.0-py2.py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 11.0MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.15.0)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.1.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs>=0.1.6->detectron2==0.1) (5.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (2.4.5)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (0.10.0)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (1.17.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (2.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (0.16.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (1.12.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (0.33.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (0.8.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (42.0.1)\n","Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (1.15.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (3.1.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (3.10.0)\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: Pillow, tqdm, detectron2\n","  Found existing installation: Pillow 4.3.0\n","    Uninstalling Pillow-4.3.0:\n","      Successfully uninstalled Pillow-4.3.0\n","  Found existing installation: tqdm 4.28.1\n","    Uninstalling tqdm-4.28.1:\n","      Successfully uninstalled tqdm-4.28.1\n","  Running setup.py develop for detectron2\n","Successfully installed Pillow-6.2.1 detectron2 tqdm-4.40.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","tqdm"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"u8MVJBHOrriU","colab_type":"code","colab":{}},"source":["# You may need to restart your runtime prior to this, to let your installation take effect\n","# Some basic setup\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ca2zgruorw8k","colab_type":"code","colab":{}},"source":["from detectron2.data.datasets import register_coco_instances\n","register_coco_instances(\"just_nuts_3\", {}, \"/content/drive/My Drive/Colab Notebooks/cocodataset/train/trainval.json\", \"/content/drive/My Drive/Colab Notebooks/cocodataset/train/images\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wEqWQ23asA1u","colab_type":"code","outputId":"43690cdf-7cc4-4d52-bea9-9f67ba790cf3","executionInfo":{"status":"ok","timestamp":1575584348023,"user_tz":-60,"elapsed":1328,"user":{"displayName":"sreeni vasa hv","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGRGILT9ymLBiio6LAzf9v1Yur0IFEmPAOx9tG-0I=s64","userId":"10395202719377635148"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["just_nuts_metadata = MetadataCatalog.get(\"just_nuts_3\")\n","dataset_dicts = DatasetCatalog.get(\"just_nuts_3\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/05 22:19:07 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[12/05 22:19:07 d2.data.datasets.coco]: \u001b[0mLoaded 300 images in COCO format from /content/drive/My Drive/Colab Notebooks/cocodataset/train/trainval.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"13SnaFfasm0c","colab_type":"code","outputId":"d62f12c5-8ea8-4256-d5a9-01b49249e6c5","executionInfo":{"status":"ok","timestamp":1575584357609,"user_tz":-60,"elapsed":5718,"user":{"displayName":"sreeni vasa hv","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGRGILT9ymLBiio6LAzf9v1Yur0IFEmPAOx9tG-0I=s64","userId":"10395202719377635148"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1VHYW3Xe2-2496gxANi7ZgnoUm1ucSVUx"}},"source":["import random\n","\n","for d in random.sample(dataset_dicts, 3):\n","    img = cv2.imread(d[\"file_name\"])\n","    visualizer = Visualizer(img[:, :, ::-1], metadata=just_nuts_metadata, scale=0.5)\n","    vis = visualizer.draw_dataset_dict(d)\n","    cv2_imshow(vis.get_image()[:, :, ::-1])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"Vmpx_lP4s3Wr","colab_type":"code","outputId":"e703b4bb-10cd-46fb-b4ff-5a4649677f40","executionInfo":{"status":"ok","timestamp":1575587682909,"user_tz":-60,"elapsed":125798,"user":{"displayName":"sreeni vasa hv","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGRGILT9ymLBiio6LAzf9v1Yur0IFEmPAOx9tG-0I=s64","userId":"10395202719377635148"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from detectron2.engine import DefaultTrainer\n","from detectron2.config import get_cfg\n","import os\n","\n","cfg = get_cfg()\n","cfg.merge_from_file(\"./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml\")\n","cfg.DATASETS.TRAIN = (\"just_nuts_2\",)\n","cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x/137849525/model_final_4ce675.pkl\"  # initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.02\n","cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # 3 classes (data, fig, hazelnut)\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg)\n","trainer.resume_or_load(resume=False)\n","trainer.train()"],"execution_count":28,"outputs":[{"output_type":"stream","text":["\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/05 23:12:37 d2.config.compat]: \u001b[0mConfig './detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n","\u001b[32m[12/05 23:12:37 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): ResNet(\n","    (stem): BasicStem(\n","      (conv1): Conv2d(\n","        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","      )\n","    )\n","    (res2): Sequential(\n","      (0): BottleneckBlock(\n","        (shortcut): Conv2d(\n","          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv1): Conv2d(\n","          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","      )\n","      (1): BottleneckBlock(\n","        (conv1): Conv2d(\n","          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","      )\n","      (2): BottleneckBlock(\n","        (conv1): Conv2d(\n","          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","      )\n","    )\n","    (res3): Sequential(\n","      (0): BottleneckBlock(\n","        (shortcut): Conv2d(\n","          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv1): Conv2d(\n","          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","      )\n","      (1): BottleneckBlock(\n","        (conv1): Conv2d(\n","          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","      )\n","      (2): BottleneckBlock(\n","        (conv1): Conv2d(\n","          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","      )\n","      (3): BottleneckBlock(\n","        (conv1): Conv2d(\n","          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","      )\n","    )\n","    (res4): Sequential(\n","      (0): BottleneckBlock(\n","        (shortcut): Conv2d(\n","          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","        (conv1): Conv2d(\n","          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (1): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (2): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (3): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (4): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (5): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (objectness_logits): Conv2d(1024, 15, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(1024, 60, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): Res5ROIHeads(\n","    (pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (res5): Sequential(\n","      (0): BottleneckBlock(\n","        (shortcut): Conv2d(\n","          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","        )\n","        (conv1): Conv2d(\n","          1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","        )\n","      )\n","      (1): BottleneckBlock(\n","        (conv1): Conv2d(\n","          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","        )\n","      )\n","      (2): BottleneckBlock(\n","        (conv1): Conv2d(\n","          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","        )\n","      )\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=2048, out_features=4, bias=True)\n","      (bbox_pred): Linear(in_features=2048, out_features=12, bias=True)\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (deconv): ConvTranspose2d(2048, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (predictor): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/05 23:12:37 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[12/05 23:12:37 d2.data.datasets.coco]: \u001b[0mLoaded 100 images in COCO format from /content/data/trainval.json\n","\u001b[32m[12/05 23:12:37 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 100 images left.\n","\u001b[32m[12/05 23:12:37 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[12/05 23:12:37 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"],"name":"stdout"},{"output_type":"stream","text":["'roi_heads.box_predictor.cls_score.weight' has shape (81, 2048) in the checkpoint but (4, 2048) in the model! Skipped.\n","'roi_heads.box_predictor.cls_score.bias' has shape (81,) in the checkpoint but (4,) in the model! Skipped.\n","'roi_heads.box_predictor.bbox_pred.weight' has shape (320, 2048) in the checkpoint but (12, 2048) in the model! Skipped.\n","'roi_heads.box_predictor.bbox_pred.bias' has shape (320,) in the checkpoint but (12,) in the model! Skipped.\n","'roi_heads.mask_head.predictor.weight' has shape (80, 256, 1, 1) in the checkpoint but (3, 256, 1, 1) in the model! Skipped.\n","'roi_heads.mask_head.predictor.bias' has shape (80,) in the checkpoint but (3,) in the model! Skipped.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[12/05 23:12:38 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n","\u001b[32m[12/05 23:12:47 d2.utils.events]: \u001b[0meta: 0:01:57  iter: 19  total_loss: 2.379  loss_cls: 0.813  loss_box_reg: 0.803  loss_mask: 0.650  loss_rpn_cls: 0.068  loss_rpn_loc: 0.008  time: 0.4154  data_time: 0.0299  lr: 0.000400  max_mem: 4407M\n","\u001b[32m[12/05 23:12:55 d2.utils.events]: \u001b[0meta: 0:01:47  iter: 39  total_loss: 1.493  loss_cls: 0.323  loss_box_reg: 0.812  loss_mask: 0.315  loss_rpn_cls: 0.010  loss_rpn_loc: 0.006  time: 0.4085  data_time: 0.0055  lr: 0.000799  max_mem: 4407M\n","\u001b[32m[12/05 23:13:03 d2.utils.events]: \u001b[0meta: 0:01:37  iter: 59  total_loss: 0.870  loss_cls: 0.102  loss_box_reg: 0.574  loss_mask: 0.152  loss_rpn_cls: 0.005  loss_rpn_loc: 0.007  time: 0.4068  data_time: 0.0066  lr: 0.001199  max_mem: 4407M\n","\u001b[32m[12/05 23:13:11 d2.utils.events]: \u001b[0meta: 0:01:28  iter: 79  total_loss: 0.609  loss_cls: 0.064  loss_box_reg: 0.386  loss_mask: 0.125  loss_rpn_cls: 0.004  loss_rpn_loc: 0.006  time: 0.4050  data_time: 0.0123  lr: 0.001598  max_mem: 4407M\n","\u001b[32m[12/05 23:13:19 d2.utils.events]: \u001b[0meta: 0:01:21  iter: 99  total_loss: 0.463  loss_cls: 0.069  loss_box_reg: 0.282  loss_mask: 0.110  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  time: 0.4075  data_time: 0.0039  lr: 0.001998  max_mem: 4407M\n","\u001b[32m[12/05 23:13:27 d2.utils.events]: \u001b[0meta: 0:01:13  iter: 119  total_loss: 0.432  loss_cls: 0.059  loss_box_reg: 0.261  loss_mask: 0.109  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  time: 0.4073  data_time: 0.0070  lr: 0.002398  max_mem: 4407M\n","\u001b[32m[12/05 23:13:35 d2.utils.events]: \u001b[0meta: 0:01:04  iter: 139  total_loss: 0.436  loss_cls: 0.060  loss_box_reg: 0.268  loss_mask: 0.104  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  time: 0.4064  data_time: 0.0051  lr: 0.002797  max_mem: 4407M\n","\u001b[32m[12/05 23:13:43 d2.utils.events]: \u001b[0meta: 0:00:56  iter: 159  total_loss: 0.508  loss_cls: 0.059  loss_box_reg: 0.321  loss_mask: 0.110  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  time: 0.4053  data_time: 0.0060  lr: 0.003197  max_mem: 4407M\n","\u001b[32m[12/05 23:13:52 d2.utils.events]: \u001b[0meta: 0:00:49  iter: 179  total_loss: 0.377  loss_cls: 0.042  loss_box_reg: 0.231  loss_mask: 0.106  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  time: 0.4064  data_time: 0.0167  lr: 0.003596  max_mem: 4407M\n","\u001b[32m[12/05 23:14:00 d2.utils.events]: \u001b[0meta: 0:00:41  iter: 199  total_loss: 0.450  loss_cls: 0.059  loss_box_reg: 0.263  loss_mask: 0.114  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  time: 0.4064  data_time: 0.0084  lr: 0.003996  max_mem: 4407M\n","\u001b[32m[12/05 23:14:08 d2.utils.events]: \u001b[0meta: 0:00:32  iter: 219  total_loss: 0.394  loss_cls: 0.051  loss_box_reg: 0.239  loss_mask: 0.101  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  time: 0.4057  data_time: 0.0060  lr: 0.004396  max_mem: 4407M\n","\u001b[32m[12/05 23:14:16 d2.utils.events]: \u001b[0meta: 0:00:24  iter: 239  total_loss: 0.380  loss_cls: 0.040  loss_box_reg: 0.225  loss_mask: 0.110  loss_rpn_cls: 0.001  loss_rpn_loc: 0.008  time: 0.4062  data_time: 0.0058  lr: 0.004795  max_mem: 4407M\n","\u001b[32m[12/05 23:14:24 d2.utils.events]: \u001b[0meta: 0:00:16  iter: 259  total_loss: 0.402  loss_cls: 0.053  loss_box_reg: 0.243  loss_mask: 0.100  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  time: 0.4056  data_time: 0.0051  lr: 0.005195  max_mem: 4407M\n","\u001b[32m[12/05 23:14:32 d2.utils.events]: \u001b[0meta: 0:00:08  iter: 279  total_loss: 0.391  loss_cls: 0.045  loss_box_reg: 0.228  loss_mask: 0.104  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  time: 0.4054  data_time: 0.0109  lr: 0.005594  max_mem: 4407M\n","\u001b[32m[12/05 23:14:41 d2.utils.events]: \u001b[0meta: 0:00:00  iter: 299  total_loss: 0.478  loss_cls: 0.057  loss_box_reg: 0.288  loss_mask: 0.109  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  time: 0.4057  data_time: 0.0070  lr: 0.005994  max_mem: 4407M\n","\u001b[32m[12/05 23:14:42 d2.engine.hooks]: \u001b[0mOverall training speed: 297 iterations in 0:02:00 (0.4071 s / it)\n","\u001b[32m[12/05 23:14:42 d2.engine.hooks]: \u001b[0mTotal training time: 0:02:02 (0:00:01 on hooks)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["OrderedDict()"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"c6CfysDdAuWn","colab_type":"code","colab":{}},"source":["cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n","cfg.DATASETS.TEST = (\"just_nuts_3\", )\n","predictor = DefaultPredictor(cfg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CSq9_NCdBELL","colab_type":"code","outputId":"fceac77c-c1ad-401a-e66b-3afb9cd0215d","executionInfo":{"status":"ok","timestamp":1575584513461,"user_tz":-60,"elapsed":7437,"user":{"displayName":"sreeni vasa hv","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGRGILT9ymLBiio6LAzf9v1Yur0IFEmPAOx9tG-0I=s64","userId":"10395202719377635148"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1AJOAziOnIqQkPNw6H3kYkSibmIe743dU"}},"source":["from detectron2.utils.visualizer import ColorMode\n","\n","for d in random.sample(dataset_dicts, 3):    \n","    im = cv2.imread(d[\"file_name\"])\n","    outputs = predictor(im)\n","    v = Visualizer(im[:, :, ::-1],\n","                   metadata=just_nuts_metadata, \n","                   scale=0.8, \n","                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n","    )\n","    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    cv2_imshow(v.get_image()[:, :, ::-1])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"MumFoARgLjco","colab_type":"code","outputId":"74e01163-1c92-4a95-b551-56c138af815e","executionInfo":{"status":"ok","timestamp":1575589158993,"user_tz":-60,"elapsed":3948,"user":{"displayName":"sreeni vasa hv","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCGRGILT9ymLBiio6LAzf9v1Yur0IFEmPAOx9tG-0I=s64","userId":"10395202719377635148"}},"colab":{"base_uri":"https://localhost:8080/","height":555}},"source":["!pip install pytorch2keras"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch2keras in /usr/local/lib/python3.6/dist-packages (0.2.3)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pytorch2keras) (0.4.2)\n","Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from pytorch2keras) (1.6.0)\n","Requirement already satisfied: onnx2keras in /usr/local/lib/python3.6/dist-packages (from pytorch2keras) (0.0.17)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch2keras) (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch2keras) (1.17.4)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from pytorch2keras) (1.15.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from pytorch2keras) (2.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->pytorch2keras) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pytorch2keras) (6.2.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx->pytorch2keras) (3.10.0)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->pytorch2keras) (3.6.6)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (1.15.1)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (1.0.8)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (0.33.6)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (0.2.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (1.11.2)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (1.15.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (1.15.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (0.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (0.1.8)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (0.8.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->pytorch2keras) (3.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->pytorch2keras) (2.8.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->pytorch2keras) (1.3.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->pytorch2keras) (5.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx->pytorch2keras) (42.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->pytorch2keras) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->pytorch2keras) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gUE2otKZbq0G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}